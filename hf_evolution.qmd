---
title: "The Evolution of the Hedge Fund"
subtitle: "Guest Lecture for MIT 18.5096<br>*Topics in Mathematics with Applications in Finance*"
author: "Jonathan Larkin"
date: "October 2, 2025"
date-format: "MMMM D, YYYY"
format:
  revealjs:
    self-contained: true
    menu: false
    slide-number: c/t
theme:
  - moon
---

## Disclaimer

```{=html}
<style type="text/css">
.reveal .slide-logo {
  bottom: 0;
  left: 24px
}

.reveal .slide-number {
  bottom: 10px;
  right: 10px;
  left: auto;
  top: auto;
}

</style>
```

This presentation is for informational purposes only and reflects my personal views and interests. It does not constitute investment advice and is not representative of any current or former employer. The information presented is based on publicly available sources. References to specific firms are for illustrative purposes only and do not imply endorsement.


## About Me

Managing Director at **Columbia Investment Management Co., LLC**, generalist allocator, Data Science and Research lead. Formerly CIO at **Quantopian**, Global Head of Equities and **Millennium Management LLC**, and Co-Head of Equity Derivatives Trading at **JPMorgan**.

- X/Twitter [@jonathanrlarkin](https://x.com/jonathanrlarkin)
- LinkedIn [linkedin.com/in/quantfinance](linkedin.com/in/quantfinance)

. . .

This presentation is available at [github.com/marketneutral/hedge_fund_evolution](github.com/marketneutral/hedge_fund_evolution).

## What Evolution?

Two trends

- Unbundling
- Human + Machine Collaboration



# Theory

## Condorcet Jury Theorem (1785)
- The *Condorcet Jury Theorem* states that if each member of a jury has a probability greater than 1/2 of making the correct decision, then as the number of jurors increases, the probability that the majority decision is correct approaches 1.

$$
P(\text{majority correct}) \to 1 \text{ as } n \to \infty \\
\iff \text{independence of errors}
$$

. . .

- e.g., `sklearn.ensemble.VotingClassifier` relies on this result.

## Boosting Weak Learners (1988)
::: {.incremental}
- Kearns, Michael. *Thoughts on Hypothesis Boosting*. 1988.
- Friedman, Jerome H. *Greedy function approximation: A gradient boosting machine*. 2001.
- Sequentially train many "weak learner" models, each focusing on the errors of the previous ones.
- Gradient boosted decision trees are the dominant approach in tabular machine learning still today.
- e.g., `sklearn.ensemble.HistGradientBoostingClassifier`, `xgboost`, `lightgbm`, `catboost`
:::

## Boosting in a Nutshell {.smaller}
::: {.incremental}
- $F_M$ is the ensemble model. After **M** rounds:
$$
F_M(x) = F_0(x) + \sum_{m=1}^M \gamma\, h_m(x)
$$
- Each round fits $h_m$ to the **negative gradient of the loss** at $F_{m-1}$, then updates:
$$
F_m(x) = F_{m-1}(x) + \gamma\, h_m(x)
$$
- $\gamma$ is the learning rate; $h_m$ is a weak learner (e.g., shallow tree).
:::


## Model Stacking (1992)
::: {.incremental}
- Wolpert, David H. *Stacked Generalization*. 1992.
- Train "meta-model" on the predictions of independent base models.
- Works best when base models are diverse and capture different aspects of the data.
- e.g., `sklearn.ensemble.StackingClassifier`
:::

## Stacking in a Nutshell {.smaller}
::: {.incremental}
- Combine several different models by training a **meta-model** on their predictions.
  - Train **M** independent base models $(f_1, \dots, f_M)$ (e.g., linear model, tree, neural net, etc.).
  - Using an appropriate cross validation scheme, collect **out-of-fold** predictions for each training example to avoid leakage.
  - Train a meta-model $(g)$ on these predictions (optionally with the original features).
$$
\hat{y}(x) = g\!\big(f_1(x),\, f_2(x),\, \dots,\, f_M(x)\big)
$$
:::

## Ensemble Methods Summary
::: {.incremental}
- *Voting*: combine models, majority vote.
- *Boosting*: **sequentially** build models, each correcting the previous.
- *Stacking*: combine diverse models, leveraging their strengths.
  - *Model Averaging* is a special case of stacking: the meta-model is a weighted linear sum.
:::

## Stacking into Boosting
- Why not both?
```{python}
#| eval: false
#| echo: true
#| code-line-numbers: "|5|8,9,10,11"
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

lin = Pipeline([
  ("scaler", StandardScaler()),
  ("lr", LogisticRegression(max_iter=1000))
])

stack = StackingClassifier(estimators=[("lin", lin)],
  final_estimator=LGBMClassifier(),
  stack_method="predict_proba", passthrough=True, cv=cv
)

stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)
```


## The Dunbar Number (1992)
::: {.incremental}
- Dunbar, R. I. M. (1992). *Neocortex size as a constraint on group size in primates.* Journal of Human Evolution, 22(6), 469â€“493.
- Max human maintainable stable relationships â‰ˆ 150
- Limit of trust & cohesion
- Beyond limit â†’ silos, slow decisions, culture strain
:::

## Dunbar cont'd: How Hedge Funds Manage It
::: {.incremental}
- ðŸ‘‰ Scale by respecting Dunbar  
  - *Pods* â†’ small teams, central risk  
  - *Quant* â†’ structure as an assembly line  
  - *Lean* â†’ keep a cap on size, preserve culture  
  - *Bureaucracy* â†’ heavy process to scale  
:::

## Wisdom of Crowds (2004) {.smaller}
::: {.incremental}
- Surowiecki, James. *The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations*. Doubleday, 2004.
- For the crowd to be smarter than experts, we require
  - *Diversity of opinion* â†’ different perspectives reduce blind spots
  - *Independence of members* â†’ avoid groupthink
  - *Decentralization* â†’ empower local knowledge
  - *Aggregation of information* â†’ combine insights effectively
:::

## The Common Task Framework (2007-)
::: {.incremental}
- Donoho, D. (2017). "50 Years of Data Science." *Journal of Computational and Graphical Statistics*, 26(4), 745â€“766.
  - *Define a clear task (e.g., image recognition).*
  - *Provide dataset + ground truth labels + hidden test set.*
  - *Set evaluation metric (accuracy, F1, etc.).*  
  - *Run open competition among researchers.*
- *Netflix Prize* (2006), *Kaggle* (2010), *ImageNet* (2012)...
:::

## Common Task Framework (cont'd)

- "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data", September 18, 2025, Nvidia Blog, [link.](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)

![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1536x864-png.webp){width="80%"}


## Machine, Platform, Crowd (2017)
::: {.incremental}
- Bryan McAfee and Erik Brynjolfsson. *Machine, Platform, Crowd: Harnessing Our Digital Future*. W. W. Norton & Company, 2017.
  - *Wisdom of crowd means groups > individual experts*
  - *Platforms unlock assets (Uber, Airbnb)*
  - *Innovation from open-source & collaboration*
  - *Trust via ratings (leaderboards)*
  - *Success is $f(\text{incentives}, \text{governance})$*
:::

## Theory Takeaways
::: {.incremental}
- Successes in machine learning demonstrate the critical importance of ensemble methods.
- The Common Task Framework has driven scientific progress at scale.
- Social science principles can inform on the design of incentives and processes to harness collective intelligence.
:::

# The Traditional Hedge Fund


## Quant Equity Workflow

- Larkin, Jonathan R., "A Professional Quant Equity Workflow", *Quantopian Blog*, 2016, [link.](https://github.com/marketneutral/lectures/blob/master/A%20Professional%20Quant%20Equity%20Workflow.pdf)
- Separate teams are focused along an assembly line
  - Data acquisition
  - Alpha research (aka feature engineering)
  - Signal combination (aka modeling)
  - Risk and transaction cost modeling
  - Portfolio construction (aka optimization)
  - Execution

## Quant Equity Workflow

::: columns
::: column
- Hope, Bradley. "With 125 Ph.D.s in 15 Countries, a Quant 'Alpha Factory' Hunts for Investing Edge." *Wall Street Journal*, April 5, 2017. [link](https://www.wsj.com/articles/with-125-ph-d-s-in-15-countries-a-quant-alpha-factory-hunts-for-investing-edge-1491471008)
:::

::: column
![](https://si.wsj.net/public/resources/images/BF-AP653_WORLDQ_G_20170406061514.jpg){width="95%"}
:::
:::



## Quant Equity Workflow

```{mermaid}
%%| fig-height: 4
flowchart LR

    DATA(Data) --> UDEF(Universe Definition)

    UDEF --> A1(alpha 1)
    UDEF --> A2(alpha 2)
    UDEF --> ADOTS(alpha...)
    UDEF --> AN(alpha N)

    A1 --> ACOMBO(Alpha Combination)
    A2 --> ACOMBO
    ADOTS --> ACOMBO
    AN --> ACOMBO

    DATA --> TARGET(Target)
    TARGET --> ACOMBO
    TARGET --> PCON
    DATA --> RISK(Risk & T-Cost Models)

    ACOMBO --> PCON(Optimization)
    RISK --> PCON

    PROD{{t-1 Portfolio}} --> PCON
    PCON --> IDEAL{{Ideal Portfolio}}
    IDEAL --> EXEC
    
    EXEC(Execution)
```

## Workflow: Minimal Non-Trivial Implementation

- Craft four simple alphas (momentum, reversal, quality, value)
- Create a target (forward 5d return demeaned)
- Combine alphas with linear model
- Use `cvxportfolio` machinery for risk model, t-cost model, optimization
- [Cvxportfolio repo on github](https://github.com/cvxgrp/cvxportfolio)
- Boyd, Stephen, et al. "Multiâ€‘Period Trading via Convex Optimization." *Foundations and Trends in Optimization*, vol.â€¯3, no.â€¯1, 2017, pp.â€¯1â€“76.


# Unbundling



# Human + Machine

## Types of Collaboration
- Vertical
- Horizontal
- "Bayesian"

## Vertical

## Horizontal

## Bayesian



