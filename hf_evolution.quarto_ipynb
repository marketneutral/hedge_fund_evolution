{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"The Evolution of the Hedge Fund\"\n",
        "subtitle: \"Guest Lecture for MIT 18.5096<br>*Topics in Mathematics with Applications in Finance*\"\n",
        "author: \"Jonathan Larkin\"\n",
        "date: \"October 2, 2025\"\n",
        "date-format: \"MMMM D, YYYY\"\n",
        "format:\n",
        "  revealjs:\n",
        "    self-contained: true\n",
        "    menu: false\n",
        "    slide-number: c/t\n",
        "theme:\n",
        "  - moon\n",
        "---\n",
        "\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "\n",
        "```{=html}\n",
        "<style type=\"text/css\">\n",
        ".reveal .slide-logo {\n",
        "  bottom: 0;\n",
        "  left: 24px\n",
        "}\n",
        "\n",
        ".reveal .slide-number {\n",
        "  bottom: 10px;\n",
        "  right: 10px;\n",
        "  left: auto;\n",
        "  top: auto;\n",
        "}\n",
        "\n",
        "</style>\n",
        "```\n",
        "\n",
        "\n",
        "This presentation is for informational purposes only and reflects my personal views and interests. It does not constitute investment advice and is not representative of any current or former employer. The information presented is based on publicly available sources. References to specific firms are for illustrative purposes only and do not imply endorsement.\n",
        "\n",
        "\n",
        "## About Me\n",
        "\n",
        "Managing Director at **Columbia Investment Management Co., LLC**, generalist allocator, Data Science and Research lead. Formerly CIO at **Quantopian**, Global Head of Equities and **Millennium Management LLC**, and Co-Head of Equity Derivatives Trading at **JPMorgan**.\n",
        "\n",
        "- X/Twitter [@jonathanrlarkin](https://x.com/jonathanrlarkin)\n",
        "- LinkedIn [linkedin.com/in/quantfinance](linkedin.com/in/quantfinance)\n",
        "\n",
        ". . .\n",
        "\n",
        "This presentation is available at [github.com/marketneutral/hedge_fund_evolution](github.com/marketneutral/hedge_fund_evolution).\n",
        "\n",
        "## What Evolution?\n",
        "\n",
        "Two trends\n",
        "\n",
        "- Unbundling\n",
        "- Human + Machine Collaboration\n",
        "\n",
        "\n",
        "\n",
        "# Theory\n",
        "\n",
        "## Condorcet Jury Theorem (1785)\n",
        "- The *Condorcet Jury Theorem* states that if each member of a jury has a probability greater than 1/2 of making the correct decision, then as the number of jurors increases, the probability that the majority decision is correct approaches 1.\n",
        "\n",
        "$$\n",
        "P(\\text{majority correct}) \\to 1 \\text{ as } n \\to \\infty \\\\\n",
        "\\iff \\text{independence of errors}\n",
        "$$\n",
        "\n",
        ". . .\n",
        "\n",
        "- e.g., `sklearn.ensemble.VotingClassifier` relies on this result.\n",
        "\n",
        "## Boosting Weak Learners (1988)\n",
        "::: {.incremental}\n",
        "- Kearns, Michael. *Thoughts on Hypothesis Boosting*. 1988.\n",
        "- Friedman, Jerome H. *Greedy function approximation: A gradient boosting machine*. 2001.\n",
        "- Sequentially train many \"weak learner\" models, each focusing on the errors of the previous ones.\n",
        "- Gradient boosted decision trees are the dominant approach in tabular machine learning still today.\n",
        "- e.g., `sklearn.ensemble.HistGradientBoostingClassifier`, `xgboost`, `lightgbm`, `catboost`\n",
        ":::\n",
        "\n",
        "## Boosting in a Nutshell {.smaller}\n",
        "::: {.incremental}\n",
        "- $F_M$ is the ensemble model. After **M** rounds:\n",
        "$$\n",
        "F_M(x) = F_0(x) + \\sum_{m=1}^M \\gamma\\, h_m(x)\n",
        "$$\n",
        "- Each round fits $h_m$ to the **negative gradient of the loss** at $F_{m-1}$, then updates:\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\gamma\\, h_m(x)\n",
        "$$\n",
        "- $\\gamma$ is the learning rate; $h_m$ is a weak learner (e.g., shallow tree).\n",
        ":::\n",
        "\n",
        "\n",
        "## Model Stacking (1992)\n",
        "::: {.incremental}\n",
        "- Wolpert, David H. *Stacked Generalization*. 1992.\n",
        "- Train \"meta-model\" on the predictions of independent base models.\n",
        "- Works best when base models are diverse and capture different aspects of the data.\n",
        "- e.g., `sklearn.ensemble.StackingClassifier`\n",
        ":::\n",
        "\n",
        "## Stacking in a Nutshell {.smaller}\n",
        "::: {.incremental}\n",
        "- Combine several different models by training a **meta-model** on their predictions.\n",
        "  - Train **M** independent base models $(f_1, \\dots, f_M)$ (e.g., linear model, tree, neural net, etc.).\n",
        "  - Using an appropriate cross validation scheme, collect **out-of-fold** predictions for each training example to avoid leakage.\n",
        "  - Train a meta-model $(g)$ on these predictions (optionally with the original features).\n",
        "$$\n",
        "\\hat{y}(x) = g\\!\\big(f_1(x),\\, f_2(x),\\, \\dots,\\, f_M(x)\\big)\n",
        "$$\n",
        ":::\n",
        "\n",
        "## Ensemble Methods Summary\n",
        "::: {.incremental}\n",
        "- *Voting*: combine models, majority vote.\n",
        "- *Boosting*: **sequentially** build models, each correcting the previous.\n",
        "- *Stacking*: combine diverse models, leveraging their strengths.\n",
        "  - *Model Averaging* is a special case of stacking: the meta-model is a weighted linear sum.\n",
        ":::\n",
        "\n",
        "## Stacking into Boosting\n",
        "- Why not both?"
      ],
      "id": "6248433f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "#| code-line-numbers: \"|5|8,9,10,11\"\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "lin = Pipeline([\n",
        "  (\"scaler\", StandardScaler()),\n",
        "  (\"lr\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "stack = StackingClassifier(estimators=[(\"lin\", lin)],\n",
        "  final_estimator=LGBMClassifier(),\n",
        "  stack_method=\"predict_proba\", passthrough=True, cv=cv\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "y_pred = stack.predict(X_test)"
      ],
      "id": "2c0be432",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Dunbar Number (1992)\n",
        "::: {.incremental}\n",
        "- Dunbar, R. I. M. (1992). *Neocortex size as a constraint on group size in primates.* Journal of Human Evolution, 22(6), 469â€“493.\n",
        "- Max human maintainable stable relationships â‰ˆ 150\n",
        "- Limit of trust & cohesion\n",
        "- Beyond limit â†’ silos, slow decisions, culture strain\n",
        ":::\n",
        "\n",
        "## Dunbar cont'd: How Hedge Funds Manage It\n",
        "::: {.incremental}\n",
        "- ðŸ‘‰ Scale by respecting Dunbar  \n",
        "  - *Pods* â†’ small teams, central risk  \n",
        "  - *Quant* â†’ structure as an assembly line  \n",
        "  - *Lean* â†’ keep a cap on size, preserve culture  \n",
        "  - *Bureaucracy* â†’ heavy process to scale  \n",
        ":::\n",
        "\n",
        "## Wisdom of Crowds (2004) {.smaller}\n",
        "::: {.incremental}\n",
        "- Surowiecki, James. *The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations*. Doubleday, 2004.\n",
        "- For the crowd to be smarter than experts, we require\n",
        "  - *Diversity of opinion* â†’ different perspectives reduce blind spots\n",
        "  - *Independence of members* â†’ avoid groupthink\n",
        "  - *Decentralization* â†’ empower local knowledge\n",
        "  - *Aggregation of information* â†’ combine insights effectively\n",
        ":::\n",
        "\n",
        "## The Common Task Framework (2007-)\n",
        "::: {.incremental}\n",
        "- Donoho, D. (2017). \"50 Years of Data Science.\" *Journal of Computational and Graphical Statistics*, 26(4), 745â€“766.\n",
        "  - *Define a clear task (e.g., image recognition).*\n",
        "  - *Provide dataset + ground truth labels + hidden test set.*\n",
        "  - *Set evaluation metric (accuracy, F1, etc.).*  \n",
        "  - *Run open competition among researchers.*\n",
        "- *Netflix Prize* (2006), *Kaggle* (2010), *ImageNet* (2012)...\n",
        ":::\n",
        "\n",
        "## Common Task Framework (cont'd)\n",
        "\n",
        "- \"The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\", September 18, 2025, Nvidia Blog, [link.](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n",
        "\n",
        "![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1536x864-png.webp){width=\"80%\"}\n",
        "\n",
        "\n",
        "## Machine, Platform, Crowd (2017)\n",
        "::: {.incremental}\n",
        "- Bryan McAfee and Erik Brynjolfsson. *Machine, Platform, Crowd: Harnessing Our Digital Future*. W. W. Norton & Company, 2017.\n",
        "  - *Wisdom of crowd means groups > individual experts*\n",
        "  - *Platforms unlock assets (Uber, Airbnb)*\n",
        "  - *Innovation from open-source & collaboration*\n",
        "  - *Trust via ratings (leaderboards)*\n",
        "  - *Success is $f(\\text{incentives}, \\text{governance})$*\n",
        ":::\n",
        "\n",
        "## Theory Takeaways\n",
        "::: {.incremental}\n",
        "- Successes in machine learning demonstrate the critical importance of ensemble methods.\n",
        "- The Common Task Framework has driven scientific progress at scale.\n",
        "- Social science principles can inform on the design of incentives and processes to harness collective intelligence.\n",
        ":::\n",
        "\n",
        "# The Traditional Hedge Fund\n",
        "\n",
        "\n",
        "## Quant Equity Workflow\n",
        "\n",
        "- Larkin, Jonathan R., \"A Professional Quant Equity Workflow\", *Quantopian Blog*, 2016, [link.](https://github.com/marketneutral/lectures/blob/master/A%20Professional%20Quant%20Equity%20Workflow.pdf)\n",
        "- Separate teams are focused along an assembly line\n",
        "  - Data acquisition\n",
        "  - Alpha research (aka feature engineering)\n",
        "  - Signal combination (aka modeling)\n",
        "  - Risk and transaction cost modeling\n",
        "  - Portfolio construction (aka optimization)\n",
        "  - Execution\n",
        "\n",
        "## Quant Equity Workflow\n",
        "\n",
        "::: columns\n",
        "::: column\n",
        "- Hope, Bradley. \"With 125 Ph.D.s in 15 Countries, a Quant 'Alpha Factory' Hunts for Investing Edge.\" *Wall Street Journal*, April 5, 2017. [link](https://www.wsj.com/articles/with-125-ph-d-s-in-15-countries-a-quant-alpha-factory-hunts-for-investing-edge-1491471008)\n",
        ":::\n",
        "\n",
        "::: column\n",
        "![](https://si.wsj.net/public/resources/images/BF-AP653_WORLDQ_G_20170406061514.jpg){width=\"95%\"}\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Quant Equity Workflow\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "%%| fig-height: 4\n",
        "flowchart LR\n",
        "\n",
        "    DATA(Data) --> UDEF(Universe Definition)\n",
        "\n",
        "    UDEF --> A1(alpha 1)\n",
        "    UDEF --> A2(alpha 2)\n",
        "    UDEF --> ADOTS(alpha...)\n",
        "    UDEF --> AN(alpha N)\n",
        "\n",
        "    A1 --> ACOMBO(Alpha Combination)\n",
        "    A2 --> ACOMBO\n",
        "    ADOTS --> ACOMBO\n",
        "    AN --> ACOMBO\n",
        "\n",
        "    DATA --> TARGET(Target)\n",
        "    TARGET --> ACOMBO\n",
        "    TARGET --> PCON\n",
        "    DATA --> RISK(Risk & T-Cost Models)\n",
        "\n",
        "    ACOMBO --> PCON(Optimization)\n",
        "    RISK --> PCON\n",
        "\n",
        "    PROD{{t-1 Portfolio}} --> PCON\n",
        "    PCON --> IDEAL{{Ideal Portfolio}}\n",
        "    IDEAL --> EXEC\n",
        "    \n",
        "    EXEC(Execution)\n",
        "```\n",
        "\n",
        "\n",
        "## Workflow: Minimal Non-Trivial Implementation\n",
        "\n",
        "- Craft four simple alphas (momentum, reversal, quality, value)\n",
        "- Create a target (forward 5d return demeaned)\n",
        "- Combine alphas with linear model\n",
        "- Use `cvxportfolio` machinery for risk model, t-cost model, optimization\n",
        "- [Cvxportfolio repo on github](https://github.com/cvxgrp/cvxportfolio)\n",
        "- Boyd, Stephen, et al. \"Multiâ€‘Period Trading via Convex Optimization.\" *Foundations and Trends in Optimization*, vol.â€¯3, no.â€¯1, 2017, pp.â€¯1â€“76.\n"
      ],
      "id": "dad0d6f1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: true\n",
        "#| echo: false\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import cvxportfolio as cvx\n",
        "from typing import Dict, List\n",
        "import yfinance as yf\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "def xsec_z(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Cross-sectional z-score by date.\"\"\"\n",
        "    m, s = df.mean(1), df.std(1).replace(0, 1)\n",
        "    return df.sub(m, 0).div(s, 0)\n",
        "\n",
        "def cs_demean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Cross-sectional demean by date.\"\"\"\n",
        "    return df.sub(df.mean(1), 0)\n",
        "\n",
        "def make_panel(features: Dict[str, pd.DataFrame], target: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Wide (dateÃ—asset) â†’ long panel with features + target.\"\"\"\n",
        "    X = pd.concat(features, axis=1)  # MultiIndex columns: (feat, asset)\n",
        "    X = X.stack().rename_axis(['date','asset']).reset_index()\n",
        "    Y = target.stack().rename('y').reset_index()\n",
        "    return X.merge(Y, on=['date','asset']).dropna()\n",
        "\n",
        "class ReturnsFromDF:\n",
        "    \"\"\"Forecaster wrapper for cvxportfolio (date Ã— asset DataFrame).\"\"\"\n",
        "    def __init__(self, df: pd.DataFrame): self.df = df\n",
        "    def __call__(self, t, h, universe, **k):\n",
        "        # Robust to missing dates (e.g., holidays) and assets\n",
        "        if t not in self.df.index:\n",
        "            return pd.Series(0.0, index=universe, dtype=float)\n",
        "        return self.df.loc[t].reindex(universe).fillna(0.0)\n",
        "\n",
        "assets = ['AAPL','AMZN','TSLA','GM','CVX','NKE']\n",
        "\n",
        "# Try loading from cached CSV first\n",
        "try:\n",
        "    data = pd.read_csv('hf_evolution_data.csv', header=[0,1], index_col=0, parse_dates=True)\n",
        "    prices = data['Close'].dropna(how='all')\n",
        "except FileNotFoundError:\n",
        "    # Prices (Adj Close)\n",
        "    data = yf.download(assets, start=\"2015-01-01\", group_by='column', progress=False)\n",
        "    data.to_csv('hf_evolution_data.csv')  # cache for next time\n",
        "\n",
        "prices = data['Close'].dropna(how='all')\n",
        "\n",
        "# Example price-only signals:\n",
        "mom = prices.pct_change(60)             # 3m momentum (approx)\n",
        "rev = -prices.pct_change(5)             # 1w reversal proxy\n",
        "qual = -prices.pct_change().rolling(60).std()  # \"quality\" proxy = low vol\n",
        "\n",
        "# Map to your earlier variable names (so slides run unchanged)\n",
        "btp  = (prices.rolling(252).mean() / prices)  # crude \"value\" proxy\n",
        "roa  = qual\n",
        "\n",
        "val  = btp[assets]  # value\n",
        "qual = roa[assets]  # quality\n",
        "rev  = rev[assets]  # reversal\n",
        "\n",
        "# Cross-sectional z-scoring\n",
        "mom_z, val_z, qual_z, rev_z = map(xsec_z, [mom, val, qual, rev])\n",
        "\n",
        "# We have daily data; target = forward 5d return, demeaned\n",
        "# First, rolling cumulative return over next 5 days\n",
        "r5 = prices[assets].pct_change().rolling(5).sum().shift(-5)\n",
        "y_cs = cs_demean(r5)\n",
        "\n",
        "# Index alignment (avoid silent misalignment)\n",
        "idx = (mom_z.index\n",
        "  .intersection(val_z.index)\n",
        "  .intersection(qual_z.index)\n",
        "  .intersection(rev_z.index)\n",
        "  .intersection(y_cs.index))\n",
        "\n",
        "mom_z, val_z, qual_z, rev_z, y_cs = \\\n",
        "  mom_z.loc[idx], val_z.loc[idx], qual_z.loc[idx], rev_z.loc[idx], y_cs.loc[idx]\n",
        "\n",
        "# Feature dict (single source of truth for names/order)\n",
        "FEATS = {'mom': mom_z, 'val': val_z, 'qual': qual_z, 'rev': rev_z}\n",
        "\n",
        "# Winsorize features (5thâ€“95th percentile per date)\n",
        "for k in FEATS:\n",
        "    FEATS[k] = FEATS[k].apply(lambda row:\n",
        "        pd.Series(winsorize(row, limits=[0.05, 0.05]), index=row.index),\n",
        "        axis=1\n",
        "    )"
      ],
      "id": "f028f864",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for each feature, quintile it by day\n",
        "import pdb; pdb.set_trace()\n",
        "\n",
        "panel = make_panel(FEATS, y_cs)\n",
        "\n",
        "qtiles = {}\n",
        "for k in FEATS:\n",
        "    qtiles[k] = FEATS[k].apply(lambda row: pd.qcut(row, 5, labels=False), axis=1)\n",
        "\n",
        "# plot the grouped mean returns for each feature\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
        "axs = axs.flatten()\n",
        "for i, k in enumerate(FEATS):\n",
        "    mean_ret = y_cs.groupby(qtiles[k], axis=0).mean()\n",
        "    mean_ret.plot(ax=axs[i], title=k)\n",
        "    axs[i].set_xlabel('Quintile')\n",
        "    axs[i].set_ylabel('Mean Forward 5d Return (Demeaned)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "9153e781",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow: Alpha Combination\n"
      ],
      "id": "51d099bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "\n",
        "def walk_forward_oof(panel: pd.DataFrame, feature_cols: List[str],\n",
        "                     assets: List[str], warm: int = 60) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Expanding fit: train on dates < t, predict on date == t (out-of-sample).\n",
        "    Returns alpha (dateÃ—asset). Gracefully handles short histories.\n",
        "    \"\"\"\n",
        "    dates = np.sort(panel['date'].unique())\n",
        "    alpha = pd.DataFrame(index=dates, columns=assets, dtype=float)\n",
        "    model = LinearRegression()\n",
        "\n",
        "    if len(dates) <= max(warm, 1):  # not enough data to train\n",
        "        return alpha.fillna(0.0)\n",
        "\n",
        "    for i, t in enumerate(dates):\n",
        "        if i < warm:\n",
        "            continue\n",
        "        train = panel[panel['date'] < t]\n",
        "        test  = panel[panel['date'] == t]\n",
        "        if len(train) == 0 or test.empty:\n",
        "            continue\n",
        "        model.fit(train[feature_cols], train['y'])\n",
        "        alpha.loc[t, test['asset'].values] = model.predict(test[feature_cols])\n",
        "\n",
        "    return alpha.fillna(0.0)\n",
        "\n",
        "panel = make_panel(FEATS, y_cs)\n",
        "alpha = walk_forward_oof(panel, list(FEATS.keys()), assets, warm=60)\n",
        "\n",
        "# Stabilize: winsorize (5thâ€“95th percentile per date)\n",
        "alpha = alpha.apply(lambda row: \n",
        "    pd.Series(winsorize(row, limits=[0.05, 0.05]), index=row.index),\n",
        "    axis=1\n",
        ")"
      ],
      "id": "3b716d0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow: Optimization, Execution\n"
      ],
      "id": "10a195b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "#| echo: true\n",
        "rf = ReturnsFromDF(alpha)\n",
        "gamma = 3.0\n",
        "kappa = 0.05\n",
        "\n",
        "obj = (cvx.ReturnsForecast(forecaster=rf)\n",
        "  - gamma * (cvx.FullCovariance() + kappa * cvx.RiskForecastError())\n",
        "  - cvx.StocksTransactionCost()\n",
        ")\n",
        "\n",
        "constraints = [cvx.LeverageLimit(3)]\n",
        "policy = cvx.MultiPeriodOptimization(obj, constraints, planning_horizon=2)\n",
        "\n",
        "start = str(alpha.index.min().date()) if len(alpha.index) else '2020-01-01'\n",
        "sim = cvx.StockMarketSimulator(assets)\n",
        "result = sim.backtest(policy, start_time=start)"
      ],
      "id": "67bdd6a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worflow: Results!\n",
        "\n",
        "\n",
        "\n",
        "# Unbundling\n",
        "\n",
        "\n",
        "\n",
        "# Human + Machine\n",
        "\n",
        "## Types of Collaboration\n",
        "- Vertical\n",
        "- Horizontal\n",
        "- \"Bayesian\"\n",
        "\n",
        "## Vertical\n",
        "\n",
        "## Horizontal\n",
        "\n",
        "## Bayesian\n"
      ],
      "id": "ab2824fa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/jlarkin/.conda/envs/nb_test/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}